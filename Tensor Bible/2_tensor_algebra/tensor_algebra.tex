\chapter{Tensor Algebra}
\label{chapter:Tensor-Algebra}

%&& a &= b & \text{Proof \ref{proof:}} \label{equation:}

Scalars are so-called zeroth-order tensors, vectors are so-called first-order tensors, and any other kind of tensor can be constructed from these.

\section{Abstract Definitions}

start with linear operators, show component representation, introduce tensor product spaces with contraction operation and co/contra basis  , explain how other operations follow, introduce isomorphisms between everything

\subsection{Linear Operators}

\begin{definition}[Linear Transformation]
	Given two vector spaces $V, W$ over the same field $\mathbb{F}$, a map $T: V \rightarrow W$ is a linear transformation if it has the following properties.
	\begin{enumerate}
		\item Additive
		\begin{equation}
			\forall \vb{x}, \vb{y} \in V: T(\vb{x} + \vb{y}) = T(\vb{x}) + T(\vb{y})
		\end{equation}
		\item Homogeneous
		\begin{equation}
			\forall \alpha \in \mathbb{F}, \vb{x} \in V: T(\alpha \vb{x}) = \alpha T(\vb{x})
		\end{equation}
	\end{enumerate}
\end{definition}

By convention, a linear operator's argument parentheses may be dropped: $T \vb{x} = T(\vb{x})$.

Given a finite-dimensional domain and range, a linear transformation can be represented with components with respect to a basis, similar to a vector or covector. This means that once bases have been chosen, linear transformations can be computed purely with components.

\begin{itemize}
	\item $V$: vector space over $\mathbb{F}$, $\dim{G} = n$, with some basis $\{ \vb{e}_{1}, \dots, \vb{e}_{n} \}$
	\item $W$: vector space over $\mathbb{F}$, $\dim{G} = m$, with some basis $\{ \vb{f}_{1}, \dots, \vb{f}_{m} \}$
	\item $T: V \rightarrow W$: linear transformation
\end{itemize}

Pick any vector $\vb{v} \in V$, and let $\vb{w} = T(\vb{v})$.

\begin{align*}
	w_{i} &= \vb{f}_{i}^{*} \left( T(\vb{v}) \right) \\
	&= \vb{f}_{i}^{*} \left( T \left( \sum_{j = 1}^{n} v^{j} \vb{e}_{j} \right) \right) \\
	&= \sum_{j = 1}^{n} \vb{f}_{i}^{*} \left(T( \vb{e}_{j}) \right) v^{j} \\
	&= \sum_{j = 1}^{n} T_{ij} v^{j} \\
	T_{ij} &= \vb{f}_{i}^{*} \left(T( \vb{e}_{j}) \right)
\end{align*}

show linear operators are a vector space L(V, W)

check for upstairsing

matrix, linear algebra

Now, it's tempting to conclude $T = \sum_{i = 1}^{m} \sum_{j = 1}^{n} T_{ij} \vb{f}_{i} \vb{e}_{j}^{*}$, and that linear operators from $V$ to $W$ have some basis $\vb{f}_{i} \vb{e}_{j}^{*}$, but technically???

\subsection{Dual/Adjoint Operator}

\subsection{Transpose Operator}

maybe do some upstairs indices here

hilbert adjoint

\subsection{Bilinear and Multilinear Operators}

bilinear form gets you a tensor. Illustrate this, perhaps with a metric.

\subsection{Tensor Product Space}

aaa

\section{Dyads and Dyadics}

tensor product spaces

order and type covector-vector degree, dual = covector

\section{Contractions}

\section{Equivalence to Linear Algebra}

isomorphic to linear algebra if appropriate combination of basis/cobasis

linearity

coordinate vs non coordinate basis

components

deltas and epsilons, symbols, only up to rank 2, etc

einstein notation

tensor as operator

basis independence

isomorphism

definitions contrived to fulfill certain properties

using dyadic symbol and not

defining property of dyadic, compare to linear algebra

simple properties of each product (distributive, associative, commutative) that may depend on the operands

linear algebra comparison

computing in canonical basis

introducing the dot product

introducing the cross product

introducing the dyadic product

specifically takes two vectors

non commutative

scalar rule

simple contractions

double contractions

change of basis

invariants of vectors and tensors

symmetric, skew-symmetric

complex numbers

operations between tensors (to fit with vectors)

metric tensor is in fact identity tensor in euclidean space

\begin{itemize}
	\item $\phi, \kappa \in \mathbb{R}$
	\item $\vb{a}, \vb{b}, \vb{c}, \vb{d} \in \mathbb{R}^{n}$
	\item $\vb{x}, \vb{y} \in \mathbb{R}^{m}$
	\item $\vb*{\omega} \in \mathbb{R}^{p}$
	\item $\vb{u}, \vb{v}, \vb{w} \in \mathbb{R}^{3}$
	\item $\vb{Q}, \vb{R} \in \mathbb{R}^{m \times m}$
	\item $\vb{S}, \vb{T} \in \mathbb{R}^{m \times n}$
	\item $\vb{U} \in \mathbb{R}^{n \times p}$
	\item $\vb{A}, \vb{B} \in \mathbb{R}^{3 \times 3}$
\end{itemize}

INTRODUCING VECTORS

\begin{flalign}
	&& \vb{S} &= \sum_{i = 1}^{m} \sum_{j = 1}^{n} S_{ij} \vb{\hat{e}}_{i} \otimes \vb{\hat{e}}_{j} & \label{equation:tens} \\
	&& \vb{u} \cross \vb{v} &= \sum_{i = 1}^{3} \sum_{j = 1}^{3} \sum_{k = 1}^{3} u_{i} v_{j} \epsilon_{ijk} \vb{\hat{e}}_{k} & \\
\end{flalign}

INTRODUCING TENSORS

\begin{flalign}
	&& \left( \vb{a} \otimes \vb{x} \right) \vb{y} &:= \vb{a} \left( \vb{x} \vdot \vb{y} \right) & \label{equation:vec_dyad_vec_dot_vec} \\
	&& \vb{a} \vdot \left( \vb{b} \otimes \vb{x} \right) &:= \left( \vb{a} \vdot \vb{b} \right) \vb{x} & \label{equation:vec_dot_vec_dyad_vec} \\
	&& \vb{x} \vdot \left( \vb{y} \otimes \vb{a} \right) \vb{b} &= \dots & \text{Proof \ref{proof:vec_dot_vec_dyad_vec_dot_vec}} \label{equation:vec_dot_vec_dyad_vec_dot_vec} \\
	&& \left( \vb{a} \otimes \vb{x} \right)^{T} &:= \vb{x} \otimes \vb{a} & \\
	&& \trace \left( \vb{a} \otimes \vb{b} \right) &:= \vb{a} \vdot \vb{b} & \\
	&& \left( \vb{a} \otimes \vb{x} \right) \left( \vb{y} \otimes \vb*{\omega} \right) &= \left( \vb{x} \vdot \vb{y} \right) \left( \vb{a} \otimes \vb*{\omega} \right) & \text{Proof \ref{proof:vec_dyad_vec_dot_vec_dyad_vec}} \label{equation:vec_dyad_vec_dot_vec_dyad_vec} \\
	&& \left( \vb{a} \otimes \vb{x} \right) \vddot \left( \vb{b} \otimes \vb{y} \right) &:= \left( \vb{a} \vdot \vb{b} \right) \left( \vb{x} \vdot \vb{y} \right) & \label{equation:vec_dyad_vec_double_vec_dyad_vec} \\
	&& \vb{a} \otimes \vb{x} &= \sum_{i = 1}^{n} \sum_{j = 1}^{m} a_{i} x_{j} \vb{\hat{e}}_{i} \otimes \vb{\hat{e}}_{j} & \\
\end{flalign}

CONTRACTIONS

\begin{flalign}
	&& \vb{a} \vdot \vb{b} &= \sum_{i = 1}^{n} a_{i} b_{i} & \text{Proof \ref{proof:vec_dot_vec}} \label{equation:vec_dot_vec} \\
	&& \vb{S} \vb{a} = \vb{S} \vdot \vb{a} &= \sum_{i = 1}^{m} \sum_{j = 1}^{n} S_{ij} a_{j} \vb{\hat{e}}_{i} & \text{Proof \ref{proof:tens_dot_vec}} \label{equation:tens_dot_vec} \\
	&& \vb{x} \vdot \vb{S} &= \sum_{i = 1}^{m} \sum_{k = 1}^{n} x_{i} S_{ik} \vb{\hat{e}}_{k} & \text{Proof \ref{proof:vec_dot_tens}} \label{equation:vec_dot_tens} \\
	&& \vb{S} \vb{U} &= \sum_{i = 1}^{m} \sum_{j = 1}^{n} \sum_{\ell = 1}^{p} S_{ij} U_{j\ell} \left( \vb{\hat{e}}_{i} \otimes \vb{\hat{e}}_{\ell} \right) & \text{Proof \ref{proof:tens_dot_tens}} \label{equation:tens_dot_tens} \\
	&& \vb{S} \vddot \vb{T} &= \sum_{i = 1}^{m} \sum_{j = 1}^{n} S_{ij} T_{ij} & \text{Proof \ref{proof:tens_double_tens}} \label{equation:tens_double_tens}
\end{flalign}

\section{Miscellaneous Operations}
\label{section:miscellaneous_operations}

\subsection{Change of Basis}
\label{section:change_of_basis}

The beauty of an abstract approach to tensors is recognizing that any change-of-basis matrix corresponds to the identity tensor expressed with components in a particular basis. After all, no matter what basis you use, the geometric quantity is the same, its the components and bases that change.

\section{Helpful Identities}
\label{section:helpful_identities}

\begin{flalign}
	&& \vb{S} \vb{a} &= \vb{a} \vdot \left( \vb{S}^{T} \right) & \text{Proof \ref{proof:tens_dot_vec_2}} & \label{equation:tens_dot_vec_2} \\
	&& \left( \vb{x} \vdot \vb{S} \right) \vdot \vb{a} &= \vb{x} \vdot \left( \vb{S} \vb{a} \right) & \text{Proof \ref{proof:vec_dot_tens_dot_vec}} \label{equation:vec_dot_tens_dot_vec} \\
	&& \left( \vb{S}^{T} \right) \vddot \vb{V} &= \vb{S} \vddot \left( \vb{V}^{T} \right) & \text{Proof \ref{proof:tens_trans_double_tens}} & \label{equation:tens_trans_double_tens}
\end{flalign}