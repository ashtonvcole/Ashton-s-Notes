\chapter{Vectors}
\label{chapter:Vectors}

%&& a &= b & \text{Proof \ref{proof:}} \label{equation:}

Ironically enough, this was the hardest section to write. It is easy to get lost in abstraction and pedantics, but it is important to break away from the notion of vectors and tensors as tables of numbers. They may be represented by tables of numbers, but since vectors in continuum mechanics are geometric quantities, those components are always relative to a particular frame of reference. Careful abstraction means that operations on vectors and tensors can be defined or proven generally, and then translated to any particularly useful coordinate system or basis once it becomes necessary.

I think that the best place to start is with vectors. Tensors are always defined relative to some kind of vector space. Even sticking with vectors and tensors as tables, a matrix is only useful insofar as it operates on vectors. Otherwise, a $3 \times 3$ matrix might be better suited as a 9-dimensional vector. The two spaces are isomorphic, so the only real difference is how they are used.

\section{Vector Analysis}

bla bla bla operations past numbers which are addition-like or multiplication like

\subsection{Group-Like Structures}

Groups generalize binary operations with addition- or multiplication-like properties.

\begin{definition}[Monoid]
	A monoid $\{ M, \cdot \}$ is a set $M$ with a binary operation $\cdot: G \times G \rightarrow G$ which satisfies the following properties.
	\begin{enumerate}
		\item Associativity
		\begin{equation}
			\forall m, n, p \in M: m \cdot (n \cdot p) = (m \cdot n) \cdot p
		\end{equation}
		\item Identity element
		\begin{equation}
			\exists i \in M: \forall m \in M: i \cdot m = m \cdot i = m
		\end{equation}
	\end{enumerate}
\end{definition}

\begin{definition}[Group]
	A group $\{ G, \cdot \}$ is a monoid which also has inverse elements.
	\begin{equation}
		\forall a \in G: \exists b \in A: a \cdot b = i
	\end{equation}
\end{definition}

\begin{definition}[Abelian Group]
	An Abelian group $\{A, \cdot\}$ is a group which also has commutativity
	\begin{equation}
		\forall a, b \in A: a \cdot b = b \cdot a
	\end{equation}
\end{definition}

In the same spirit of generalizing addition and multiplication, the following two notation conventions are often used for such operations.

\begin{itemize}
	\item Additive: An addition-like operation, almost always an Abelian group, with $+$ for the operation, $0$ for the identity, and $-a$ for any inverses. For a group, subtraction can be defined.
	\begin{equation}
		a - b := a + (-b)
	\end{equation}
	\item Multiplicative: A multiplication-like operation, less often an Abelian group, with $\times$, $\cdot$, or $*$ for the operation, $1$ for the identity, and $a^{-1}$ for any inverses. For a group, division can be defined.
	\begin{equation}
		a / b := a \cdot b^{-1}
	\end{equation}
\end{itemize}

\subsection{Group Actions}

Actions generalize operations between arbitrary sets and monoids. For example, displacements are an action on positions. Compositions of displacements form an additive Abelian group using ``tip to tail'' addition. If one composition of displacements equals another composition, then they should get you to the same final destination.

\begin{definition}[Left Action]
	A left action $\circ: M \times X \rightarrow X$ is a binary operation between a monoid $\{ M, \cdot \}$ with identity $i$ and a set $X$ which satisfies the following properties.
	\begin{enumerate}
		\item Left identity
		\begin{equation}
			\forall x \in X: i \circ x = x
		\end{equation}
		\item Left compatibility
		\begin{equation}
			\forall m, n \in M, x \in X: m \circ (n \circ x) = (m \cdot n) \circ x
		\end{equation}
	\end{enumerate}
\end{definition}

\begin{definition}[Right Action]
	A right action $\circ: X \times M \rightarrow X$ is a binary operation between a set $X$ and a monoid $\{ M, \cdot \}$ with identity $i$ which satisfies the following properties.
	\begin{enumerate}
		\item Right identity
		\begin{equation}
			\forall x \in X: x \circ i = x
		\end{equation}
		\item Right compatibility
		\begin{equation}
			\forall x \in X, m, n \in G: (x \circ m) \circ n = x \circ (m \cdot n)
		\end{equation}
	\end{enumerate}
\end{definition}

\subsection{Ring-Like Structures}

Rings generalize cases where there is both an addition- and multiplication-like operation. Multiplication may or may not be invertible, but if it is, cannot inlcude the additive identity 0. See Proof \ref{proof:div_by_zero}.

\begin{definition}[Ring]
	A ring $\{ R, +, \cdot \}$ is a set $R$ with an additive Abelian group $\{ R, + \}$ and a multiplicative monoid $\{ R, \cdot \}$ which satisfy the following properties.
	\begin{enumerate}
		\item Left distributivity
		\begin{equation}
			\forall r, s, t \in R: r \cdot (s + t) = r \cdot s + r \cdot t
		\end{equation}
		\item Right distributivity
		\begin{equation}
			\forall r, s, t \in R: (r + s) \cdot t = r \cdot t + s \cdot t
		\end{equation}
	\end{enumerate}
\end{definition}

\begin{definition}[Commutative Ring]
	A commutative ring $\{ R, +, \cdot \}$ is a ring where $\{ R, \cdot \}$ is commutative.
\end{definition}

\begin{definition}[Field]
	A field $\{ \mathbb{F}, +, \cdot \}$ is a ring where $\{ \mathbb{F} \setminus 0, \cdot \}$ is a multiplicative group.
\end{definition}

\subsection{Module-Like Structures}

Modules generalize vector spaces.

\begin{definition}[Left Module]
	A left module $A$ over a ring $R$ is a system of sets and operations $\{ A, R, \oplus, +, \cdot, * \}$ which satisfy the following properties.
	\begin{enumerate}
		\item $\{ A, \oplus \}$ is an additive Abelian group.
		\item $\{ R, +, \cdot \}$ is a ring.
		\item Scalar multiplication $*: R \times A \rightarrow A$ is a left action of the multiplicative monoid $\{ R, \cdot \}$ of the ring on $A$ which satisfies the following properties.
		\begin{enumerate}
			\item Distributivity over $A$'s addition
			\begin{equation}
				\forall r \in R, a, b \in A: r * (a \oplus b) = r * a \oplus r * b
			\end{equation}
			\item Distributivity over $R$'s addition into $A$'s addition
			\begin{equation}
				\forall r, s \in R, a \in A: (r + s) * a = r * a \oplus s * a
			\end{equation}
		\end{enumerate}
	\end{enumerate}
\end{definition}

\begin{definition}[Vector Space]
	A vector space $V$ over a field $\mathbb{F}$ is a left module with sets and operations $\{V, \mathbb{F}, \oplus, +, \cdot, *\}$ where the ring is restricted to a field. Scalar multiplication is then a left action of the multiplicative group $\{ \mathbb{F} \setminus 0, \cdot \}$.
\end{definition}

Further, scalar multiplication can be used to relate the inverse and identity elements of the field and vector space.

\begin{flalign}
	&& \forall \vb{v} \in V: 0 * \vb{v} &= \vb{0} & \text{Proof \ref{proof:smult_zero}} \label{equation:smult_zero} \\
	&& \forall \vb{v} \in V: (-1) * \vb{v} &= -\vb{v} & \text{Proof \ref{proof:smult_inverse}} \label{equation:smult_inverse}
\end{flalign}

In practice, the notation used is looser. Vector and scalar addition share the same symbol. Field and scalar multiplication symbols are usually dropped. The operations can be inferred from context.

\begin{definition}[Affine Space]
	An affine space $\{ X, V, +, \oplus \}$ is a set $X$, vector space $V$ over a field $\mathbb{F}$, and a right action $\oplus: X \times V \rightarrow X$ of the additive group $\{ V, + \}$ which satisfies the following properties.
	\begin{enumerate}
		\item The action is
		\begin{enumerate}
			\item Free
			\begin{equation}
				\forall \vb{v}\in V: x \oplus \vb{v} = x \implies \vb{v} = \vb{0}
			\end{equation}
			\item Transitive
			\begin{equation}
				\forall x, y \in X: \exists \vb{v} \in V: x \oplus \vb{v} = y
			\end{equation}
		\end{enumerate}
	\end{enumerate}
\end{definition}

This generalizes spaces with no inherent reference point, such as a Euclidean space or time. Geometric locations and dates can be considered in a relative sense. Assigning an origin or calendar is arbitrary, after all. Elements in $X$ are called points and vectors in $V$ are called translations or free vectors.

The definition above also implies the following properties.

\begin{flalign}
	&& \forall x, y \in X: \exists! \vb{v} \in V: x \oplus \vb{v} &= y & \text{Bijection  for fixed $x$. Proof \ref{proof:aff_bij_x}} \label{equation:aff_bij_x} \\
	&& \forall \vb{v} \in V, y \in X: \exists! x \in X: x \oplus \vb{v} &= y & \text{Bijection for fixed $\vb{v}$. Proof \ref{proof:aff_bij_v}} \label{equation:aff_bij_v}
\end{flalign}

Since these operations are invertible, both point-vector $\ominus: X \times V \rightarrow X$ and point-point $--: X \times X \rightarrow V$ subtraction operations are well-defined (Proof \ref{proof:aff_sub}).

\begin{equation}
	a \oplus (b -- a) = b
\end{equation}

This operation also satisfies the following properties, called Weyl's axioms.

\begin{flalign}
	&& \forall x \in X, \vb{v} \in V: \exists! y \in x: y -- x &= \vb{v} & \text{Proof \ref{proof:aff_w1}} \label{equation:aff_w1} \\
	&& \forall x, y, z \in X: (c -- b) + (b -- a) &= c -- a & \text{Proof \ref{proof:aff_w2}} \label{equation:aff_w2}
\end{flalign}

Furthermore, the parallelogram property is satisfied.

\begin{equation}
	\forall w, x, y, x \in X: b -- a = d -- c \equiv c -- a = d -- b
\end{equation}

Finally, any vector space is an affine space over itself (Proof \ref{proof:vec_self_aff}). This means that any element of $V$ can be considered as a point or a vector. Importantly, this formalizes that notion of a reference point being arbitrary. Given a reference point $o \in A$, an isomorphism $\iota_{o}: X \leftrightarrow V$ can be constructed between $A$ and $V$.

\begin{equation}
	\iota_{o}(x) = x \ominus o
\end{equation}

\begin{definition}[Affine Subspace]
	An affine subspace $W \subseteq X$ of an affine space $\{ X, V, +, \oplus \}$ is a set $W$ for which there exists some $w \in W$ such that generated vectors $\overrightarrow{W} = \{ x \ominus w: x \in W \}$ is a linear subspace of V.
\end{definition}

It folllows that the choice of $w$ is arbitrary, and all subspaces can be generated from linear subspaces $U \subseteq V$ as follows.

\begin{equation}
	x + U := \{ x + \vb{u}: \vb{u} \in U \}
\end{equation}

The linear subspace $U$ can be considered the affine subspace's direction, and two subspaces that share the same or subsidiary directions $T \subseteq U$ are considered parallel. The above properties imply Playfair's axiom: for any point $x \in X$ and direction $U \subseteq V$ there is a unique affine subspace of direction $U$ (Proof \ref{proof:playfair}).

As with vector spaces, distinguishing these operations with special symbols is done for clarity. In practice, simple addition and subtraction are used, and such notation will be used moving forwards.

\subsection{Basis}

different kinds of bases, linear independence, dimension, isomorphism to Fn

\subsection{Dual Space}

algebraic, topological

\subsection{Topological Functions}

\begin{definition}[Metric]
	A metric $d: X \times X \rightarrow [0, \infty)$ is a function that generalizes distance between elements of the set $X$ by satisfying the following properties.
	\begin{enumerate}
		\item Zero property
		\begin{equation}
			\forall x, y \in X: d(x, y) = 0 \iff x = y
		\end{equation}
		\item Symmetry
		\begin{equation}
			\forall x, y \in X: d(x, y) = d(y, x)
		\end{equation}
		\item Triangle inequality
		\begin{equation}
			\forall x, y, z \in X: d(x, y) \leq d(x, z) + d(y, z)
		\end{equation}
	\end{enumerate}
\end{definition}

A set with an associated metric is called a metric space.

\begin{definition}[Norm]
	A norm $\norm{\cdot}: V \rightarrow [0, \infty)$ is a function that generalizes magnitude of vectors in a vector space $X$ over a real or complex field $\mathbb{F} \subseteq \mathbb{C}$ by satisfying the following properties.
	\begin{enumerate}
		\item Positive-definiteness
		\begin{equation}
			\forall \vb{v} \in V: \norm{\vb{v}} = 0 \iff \vb{v} = \vb{0}
		\end{equation}
		\item Absolute homogeneity, i.e. scaling property
		\begin{equation}
			\forall \alpha \in \mathbb{F}, \vb{v} \in V: \norm{\alpha \vb{v}} = \abs{\alpha} \norm{\vb{v}}
		\end{equation}
		\item Triangle inequality
		\begin{equation}
			\forall \vb{u}, \vb{v} \in V: \norm{\vb{u} + \vb{v}} \leq \norm{\vb{u}} + \norm{\vb{v}}
		\end{equation}
	\end{enumerate}
\end{definition}

A vector space with a norm is called a normed linear space. It is also a metric space using the induced metric.

\begin{flalign}
	&& d(\vb{u}, \vb{v}) &= \norm{\vb{u} - \vb{v}} & \text{Proof \ref{proof:nls_is_ms}} \label{equation:nls_is_ms}
\end{flalign}

All norms are equivalent in finite-dimensional spaces. See Proof \ref{proof:norm_equivalence}.

\begin{definition}
	An inner product $(\cdot, \cdot): V \times V \rightarrow \mathbb{F}$ is a function that generalizes the similarity of vectors in a vector space $X$ over a real or complex field $\mathbb{F} \subseteq \mathbb{C}$ by satisfying the following properties.
	\begin{enumerate}
		\item Positive-definiteness
		\begin{equation}
			\forall \vb{x} \in X: (\vb{x}, \vb{x}) = 0 \equiv \vb{X} = \vb{0}
		\end{equation}
		\item Linearity in the first argument
		\begin{equation}
			\forall \alpha_{1}, \alpha_{2} \in \mathbb{F}, \vb{x}_{1}, \vb{x}_{2}, \vb{y} \in X: (\alpha_{1} \vb{x}_{1} + \alpha_{2} \vb{x}_{2}, \vb{y}) = \alpha_{1} (\vb{x}_{1}, \vb{y}) + \alpha_{2} (\vb{x}_{2}, \vb{y})
		\end{equation}
		\item Conjugate symmetry
		\begin{equation}
			\forall \vb{x}, \vb{y} \in X: (\vb{x}, \vb{y}) = \overline{(\vb{y}, \vb{x})}
		\end{equation}
	\end{enumerate}
\end{definition}

These conditions imply anti-linearity in the second argument, or linearity for functions over a real field.

\begin{equation}
	\forall \beta_{1}, \beta_{2} \in \mathbb{F}, \vb{x}, \vb{y}_{1}, \vb{y}_{2} \in X: (\vb{x}, \beta_{1} \vb{y}_{1} + \beta_{2} \vb{y}_{2}) = \beta_{1} (\vb{x}, \vb{y}_{1}) + \beta_{2} (\vb{x}, \vb{y}_{2})
\end{equation}

For any inner product, the Cauchy-Schwarz inequality applies.

\begin{flalign}
	&& (\vb{x}, \vb{y}) &\leq \sqrt{(\vb{x}, \vb{x}) (\vb{y}, \vb{y})} & \text{Proof \ref{proof:csneq}} \label{equation:csneq}
\end{flalign}

A vector space with an inner product is called an inner product space. It is also a normed linear space and a metric space using the induced norm.

\begin{flalign}
	 && \norm{\vb{x}} &= \sqrt{(\vb{x}, \vb{x})} & \text{Proof \ref{proof:ips_is_nls}} \label{equation:ips_is_nls}
\end{flalign}

An inner product also allows for an abstract notion of angle, inspired by geometry.

\begin{equation}
	\cos(\theta) = \frac{(\vb{x}, \vb{y})}{\norm{\vb{x}} \norm{\vb{y}}}
\end{equation}

\subsection{Riesz Representation}

Riesz representation and dealing with complex vector spaces, dual basis cobasis link, also compare components

\section{Geometric Spaces}

\begin{definition}[Euclidean Point Space]
	The Euclidean point space
\end{definition}

\begin{definition}[Euclidean Vector Space]
	content...
\end{definition}

\section{Vector Spaces in Continuum Mechanics}

do a proper construction here

construction of euclidean geometry but don't get to lost

Rn is a Euclidean geometry, also cartesian coordinates

note on how velocity

also fields which are functions, algebra of fields is pointwise based, calculus is trickier

coordinate systems: cartesian, polar, spherical, navd88, wgs84

















\section{Getting Started}

For all intents and purposes of continuum mechanics, it is sufficient to start with a Euclidean space, usually two- or three-dimensional, described by a Cartesian coordinate system, because this can be used to describe the physical world at our scale quite well.

\subsection{Notation}

Scalars are simply written as a letter, usually lower-case ($a$). Vectors are bolded ($\vb{a}$). When hand-written, they may have an arrow on top ($\vec{a}$) or a single underline ($\underline{a}$). Unit vectors, with a magnitude 1, may be marked with a circumflex ($\vu{a}$). Higher-order tensors are bolded and usually upper-case ($\vb{A}$). When written, they may have a number of underlines equal to their order.

\section{Bases and Components}

We start with some right-handed orthonormal basis $\{\vu{e}_{i}\}_{i = 1}^{n}$ for an $n$-dimensional Euclidean vector space $\mathbb{R}^{n}$. If these align with a Cartesian coordinate frame, or when a Cartesian coordinate frame is defined to line up with them, they're called the canonical basis, i.e. the standard one. Using the geometric definitions for the dot and cross product, the following relations exist between the basis vectors, represented in shorthand by the Kronecker delta $\delta$ and Levi-Civita symbol $\epsilon$.

These products are both bilinear. The dot product commutes, and the cross product anti-commutes.

\begin{flalign}
	&& \vu{e}_{i} \vdot \vu{e}_{j} = \delta_{ij} &:= \begin{cases}
		1, & i = j, \\
		0, & i \neq j.
	\end{cases} & \label{equation:uvec_dot_uvec} \\
	&& \left( \vu{e}_{i} \cross \vu{e}_{j} \right) \vdot \vb{\hat{e}}_k = \epsilon_{ijk} &:= \begin{cases}
		1, & (i, j, k) = (1, 2, 3), (2, 3, 1), \mathrm{or} \,(3, 1, 2), \\
		-1, & (i, j, k) = (3, 2, 1), (2, 1, 3), \mathrm{or} \,(1, 3, 2), \\
		0, & \mathrm{otherwise.}
	\end{cases} & \label{equation:uvec_cross_uvec_dot_uvec}
\end{flalign}

Since any basis set is linearly independent and spans the vector space, any vector in the space may be represented in terms of such a basis. The coefficients of the linear combination are called components. It is important to note that these are not the same as coordinates, especially for non-Cartesian coordinate systems.

\begin{flalign}
	&& \vb{a} &= \sum_{i = 1}^{n} a_{i} \vu{e}_{i} & \label{equation:vec}
\end{flalign}

\subsection{Cobasis}

If the chosen basis is not orthonormal with respect to some inner product and corresponding induced norm, then there is some more nuance. Sidestepping the excess mental infrastructure of the dual space and Riesz representation, the cobasis of some basis $\{\vb{f}_{i}\}_{i = 1}^{n}$, marked with a superscript as $\{\vb{f}^{i}\}_{i = 1}^{n}$, is another basis for the same space, which is bi-orthonormal to the basis.

\begin{flalign}
	&& \vb{f}^{i} \vdot \vb{f}_{j} = \vb{f}_{j} \vdot \vb{f}^{i} &= \delta_{ij} &
\end{flalign}

Notation for components in a non-orthonormal basis is more complicated. The contravariant components of the basis use superscripts, and the covariant components of the cobasis use subscripts. This may seem backwards, but this is the convention that was developed.

\begin{flalign}
	&& \vb{a} &= \sum_{i = 1}^{n} \tilde{a}^{i} \vb{f}_{i} & \label{equation:vec_contra} \\
	&& \vb{a} &= \sum_{i = 1}^{n} \tilde{a}_{i} \vb{f}^{i} & \label{equation:vec_co}
\end{flalign}

Contra in contravariant refers to how the components scale inversely with the basis vectors. For example, if a basis is changed from inches to feet, the components used to represent a vector will shrink to represent the same length. On the other hand, the covariant components would increase.

If one goes further into the weeds of tensor-world, there is a whole shorthand system of upstairs and downstairs indices used for tensors defined by a certain combination of bases and cobases. However, in continuum-world, one can fudge over all of these nuances by choosing to use an orthonormal basis, which is conveniently its own cobasis.

\subsection{Independence of Basis}

That freedom of choice is an important idea behind this tensor system. Up until this point, you were probably used to seeing vectors and matrices as lists and tables of numbers. Start with a unit vector $(1, 0)$. Rotate the coordinate frame 45 degrees clockwise and you get $(\sqrt{2} / 2, \sqrt{2} / 2)$. This would seem like a new and unequal vector, since $(1, 0) \neq (\sqrt{2} / 2, \sqrt{2} / 2)$.

Writing them in terms of components of particular bases allows for more abstraction and flexibility. The vector is no longer the tuple per se, but is represented by a tuple in the context of a particular reference frame. The vector and the physical quantity it represents stay the same conceptually, regardless of the frame of reference: $\vb{a} = \sum_{i = 1}^{n} a_{i} \vu{e}_{i} = \sum_{i = 1}^{n} \tilde{a}^{i} \vb{f}_{i} = \sum_{i = 1}^{n} \tilde{a}_{i} \vb{f}^{i}$. Only components vary. So we can say for the prior example, labeling the coordinate frames as A and B, $(1, 0)_{A} = (\sqrt{2} / 2, \sqrt{2} / 2)_{B}$. Operations and properties like an inner product, cross product, norm should stay the same, too. However, it may not necessarily be that $\vb{a} \vdot \vb{b} = \sum_{i = 1}^{n} \tilde{a}^{i} \tilde{b}^{i}$, because the dot product is defined geometrically, not algebraically.

\subsection{Computing Components}

differentiate coordinates and components

\subsection{Curvilinear Coordinates}

same vector using different coordinates

\section{Kronecker Delta and Levi-Civita Symbol Identities}

The Kronecker Delta and Levi-Civita Symbol have the following identities which are useful in proof.

\begin{flalign}
	&& a &= b & \text{Proof \ref{proof:aaa}} \label{equation:aaa} \\
	&& a &= b & \text{Proof \ref{proof:bbb}} \label{equation:bbb} \\
	&& \epsilon_{pqs} \epsilon_{nrs} &= \delta_{pn} \delta_{qr} - \delta_{pr} \delta_{qn} & \text{Proof \ref{proof:eps_pqs_eps_nrs}} \label{equation:eps_pqs_eps_nrs} \\
	&& \epsilon_{pqs} \epsilon_{rqs} &= 2 \delta_{pr} & \text{Proof \ref{proof:eps_pqs_eps_rqs}} \label{equation:eps_pqs_eps_rqs}
\end{flalign}

\section{Einstein Notation}