\chapter{Vectors}
\label{chapter:Vectors}

%&& a &= b & \text{Proof \ref{proof:}} \label{equation:}

Ironically enough, this was the hardest section to write. It is easy to get lost in abstraction and pedantics, but it is important to break away from the notion of vectors and tensors as tables of numbers. They may be represented by tables of numbers, but since vectors in continuum mechanics are geometric quantities, those components are always relative to a particular frame of reference. Careful abstraction means that operations on vectors and tensors can be defined or proven generally, and then translated to any particularly useful coordinate system or basis once it becomes necessary.

I think that the best place to start is with vectors. Tensors are always defined relative to some kind of vector space. Even sticking with vectors and tensors as tables, a matrix is only useful insofar as it operates on vectors. Otherwise, a $3 \times 3$ matrix might be better suited as a 9-dimensional vector. The two spaces are isomorphic, so the only real difference is how they are used.

\section{Vector Analysis}

\subsection{Generalizing Addition and Multiplication}

\begin{definition}[Group]
	A group $\{ G, \cdot \}$ is a is a set $G$ and a binary operation $\cdot: G \times G \rightarrow G$ which satisfy the following three properties.
	\begin{enumerate}
		\item Associativity: the order in which the operation is applied does not change the result.
		\begin{equation}
			\forall a, b, c \in G: a \cdot (b \cdot c) = (a \cdot b) \cdot c
		\end{equation}
		\item Identity element: any element operated on with the identity results in itself.
		\begin{equation}
			\exists i \in G: \forall a \in A: i \cdot a = a \cdot i = a
		\end{equation}
		\item Inverse element: every element has a corresponding inverse which results in the identity.
		\begin{equation}
			\forall a \in G: \exists b \in A: a \cdot b = i
		\end{equation}
	\end{enumerate}
\end{definition}

\begin{definition}[Action]
An action $\circ: G \times X \rightarrow X$ is a binary operation between a group $\{ G, \cdot \}$ with identity $e$ and a set $X$ which satisfies the following properties.
\begin{enumerate}
	\item Left identity
	\begin{equation}
		\forall x \in X: e \circ x = x
	\end{equation}
	\item Left compatibility
	\begin{equation}
		\forall g, h \in G, x \in X: g \circ (h \circ x) = (g \cdot h) \circ x
	\end{equation}
\end{enumerate}
\end{definition}

\begin{definition}[Abelian Group]
	An Abelian group $\{A, \cdot\}$ is a group which also has commutativity: reversing the arguments does not change the result.
	\begin{equation}
		\forall a, b \in A: a \cdot b = b \cdot a
	\end{equation}
\end{definition}

Essentially, groups and Abelian groups generalize addition and multiplication past real numbers. In the same spirit, the following two notation conventions are often used for such operations.

\begin{itemize}
	\item Additive group: An addition-like group, almost always Abelian, with $+$ for the operation, $0$ for the identity, and $-a$ for the inverse.
	\item Multiplicative group: A multiplication-like group, less often Abelian, with $\times$, $\cdot$, or $*$ for the operation, $1$ for the identity, and $a^{-1}$ for the inverse.
\end{itemize}

\begin{definition}[Field]
	A field $\{ \mathbb{F}, +, \cdot \}$ is a set $\mathbb{F}$ with an addition-like operation $+: A \times A \rightarrow A$ and a multiplication-like operation $\cdot: A \times A \rightarrow A$ which satisfy the following properties.
	\begin{enumerate}
		\item $\{ \mathbb{F}, + \}$ form an additive Abelian group, with an identity $0$ and additive inverse marked as $-a$.
		\item $\{ \mathbb{F} \setminus 0, \cdot \}$ form a multiplicative Abelian group, with an identity $1$ and multiplicative inverse marked as $a^{-1}$.
		\item Distributivity of multiplication
		\begin{equation}
			\forall a, b, c \in A: a \cdot (b + c) = a \cdot b + a \cdot c
		\end{equation}
	\end{enumerate}
\end{definition}

For a field, the subtraction and division operations can also be defined.

\begin{align}
	a - b &:= a + (-b) \\
	a / b &:= a \cdot b^{-1}
\end{align}

\subsection{Vector Space}

scalar, tuples, functions dimension

\begin{definition}[Vector Space]
	A vector space $V$ over a field $\mathbb{F}$ is a system of sets and operations $\{V, \mathbb{F}, \boldsymbol{+}, +, \cdot, *\}$ which satisfy the following axioms.
	\begin{enumerate}
		\item $\{V, \boldsymbol{+}\}$ is an Abelian group with identity $\vb{0}$.
		\item $\{\mathbb{F}, +, \cdot\}$ is a field.
		\item Scalar multiplication $* : \mathbb{F} \times V \rightarrow V$ is an action of the multiplicative group of the field $\{ \mathbb{F} \setminus 0 \}$ on the vector space with the following additional properties.
		\begin{enumerate}
			\item Distributivity over vector addition
			\begin{equation}
				\forall \alpha \in \mathbb{F}, \vb{u}, \vb{v} \in V: \alpha * (\vb{u} \boldsymbol{+} \vb{v}) = \alpha * \vb{u} \boldsymbol{+} \alpha * \vb{v}
			\end{equation}
			\item Distributivity over field addition into vector addition
			\begin{equation}
				\forall \alpha, \beta \in \mathbb{F}, \vb{v} \in V: (\alpha + \beta) * \vb{v} = \alpha * \vb{v} \boldsymbol{+} \beta * \vb{v}
			\end{equation}
		\end{enumerate}
	\end{enumerate}
\end{definition}

In practice, the notation used is looser. Vector and scalar addition share the same symbol. Field and scalar multiplication symbols are usually dropped. The operations can be inferred from context.

Scalar multiplication can be used to relate the inverse and identity elements of the field and vector space.

\begin{flalign}
	&& \forall \vb{v} \in V: 0 * \vb{v} &= \vb{0} & \text{Proof \ref{proof:smult_zero}} \label{equation:smult_zero} \\
	&& \forall \vb{v} \in V: (-1) * \vb{v} &= -\vb{v} & \text{Proof \ref{proof:smult_inverse}} \label{equation:smult_inverse}
\end{flalign}

\subsection{Basis}

different kinds of bases, linear independence, dimension, isomorphism to Fn

\subsection{Dual Space}

algebraic, topological

\subsection{Topological Functions}

\begin{definition}[Metric]
	A metric $d: X \times X \rightarrow [0, \infty)$ is a function that generalizes distance between elements of the set $X$ by satisfying the following properties.
	\begin{enumerate}
		\item Zero property
		\begin{equation}
			\forall x, y \in X: d(x, y) = 0 \iff x = y
		\end{equation}
		\item Symmetry
		\begin{equation}
			\forall x, y \in X: d(x, y) = d(y, x)
		\end{equation}
		\item Triangle inequality
		\begin{equation}
			\forall x, y, z \in X: d(x, y) \leq d(x, z) + d(y, z)
		\end{equation}
	\end{enumerate}
\end{definition}

A set with an associated metric is called a metric space.

\begin{definition}[Norm]
	A norm $\norm{\cdot}: V \rightarrow [0, \infty)$ is a function that generalizes magnitude of vectors in a vector space $X$ over a real or complex field $\mathbb{F} \subseteq \mathbb{C}$ by satisfying the following properties.
	\begin{enumerate}
		\item Positive-definiteness
		\begin{equation}
			\forall \vb{v} \in V: \norm{\vb{v}} = 0 \iff \vb{v} = \vb{0}
		\end{equation}
		\item Absolute homogeneity, i.e. scaling property
		\begin{equation}
			\forall \alpha \in \mathbb{F}, \vb{v} \in V: \norm{\alpha \vb{v}} = \abs{\alpha} \norm{\vb{v}}
		\end{equation}
		\item Triangle inequality
		\begin{equation}
			\forall \vb{u}, \vb{v} \in V: \norm{\vb{u} + \vb{v}} \leq \norm{\vb{u}} + \norm{\vb{v}}
		\end{equation}
	\end{enumerate}
\end{definition}

A vector space with a norm is called a normed linear space. It is also a metric space using the induced metric.

\begin{flalign}
	&& d(\vb{u}, \vb{v}) &= \norm{\vb{u} - \vb{v}} & \text{Proof \ref{proof:nls_is_ms}} \label{equation:nls_is_ms}
\end{flalign}

All norms are equivalent in finite-dimensional spaces. See Proof \ref{proof:norm_equivalence}.

\begin{definition}
	An inner product $(\cdot, \cdot): V \times V \rightarrow \mathbb{F}$ is a function that generalizes the similarity of vectors in a vector space $X$ over a real or complex field $\mathbb{F} \subseteq \mathbb{C}$ by satisfying the following properties.
	\begin{enumerate}
		\item Positive-definiteness
		\begin{equation}
			\forall \vb{x} \in X: (\vb{x}, \vb{x}) = 0 \equiv \vb{X} = \vb{0}
		\end{equation}
		\item Linearity in the first argument
		\begin{equation}
			\forall \alpha_{1}, \alpha_{2} \in \mathbb{F}, \vb{x}_{1}, \vb{x}_{2}, \vb{y} \in X: (\alpha_{1} \vb{x}_{1} + \alpha_{2} \vb{x}_{2}, \vb{y}) = \alpha_{1} (\vb{x}_{1}, \vb{y}) + \alpha_{2} (\vb{x}_{2}, \vb{y})
		\end{equation}
		\item Conjugate symmetry
		\begin{equation}
			\forall \vb{x}, \vb{y} \in X: (\vb{x}, \vb{y}) = \overline{(\vb{y}, \vb{x})}
		\end{equation}
	\end{enumerate}
\end{definition}

These conditions imply anti-linearity in the second argument, or linearity for functions over a real field.

\begin{equation}
	\forall \beta_{1}, \beta_{2} \in \mathbb{F}, \vb{x}, \vb{y}_{1}, \vb{y}_{2} \in X: (\vb{x}, \beta_{1} \vb{y}_{1} + \beta_{2} \vb{y}_{2}) = \beta_{1} (\vb{x}, \vb{y}_{1}) + \beta_{2} (\vb{x}, \vb{y}_{2})
\end{equation}

For any inner product, the Cauchy-Schwarz inequality applies.

\begin{flalign}
	&& (\vb{x}, \vb{y}) &\leq \sqrt{(\vb{x}, \vb{x}) (\vb{y}, \vb{y})} & \text{Proof \ref{proof:csneq}} \label{equation:csneq}
\end{flalign}

A vector space with an inner product is called an inner product space. It is also a normed linear space and a metric space using the induced norm.

\begin{flalign}
	 && \norm{\vb{x}} &= \sqrt{(\vb{x}, \vb{x})} & \text{Proof \ref{proof:ips_is_nls}} \label{equation:ips_is_nls}
\end{flalign}

An inner product also allows for an abstract notion of angle, inspired by geometry.

\begin{equation}
	\cos(\theta) = \frac{(\vb{x}, \vb{y})}{\norm{\vb{x}} \norm{\vb{y}}}
\end{equation}

\subsection{Riesz Representation}

Riesz representation and dealing with complex vector spaces, dual basis cobasis link, also compare components

\section{Affine and Euclidean Spaces}

\begin{definition}[Affine Space]
	An affine space $\{ G, V, +, \oplus \}$ is an additive group $\{ G, + \}$, a vector space $V$, and an action $\oplus: A \times \overrightarrow{A} \rightarrow \overrightarrow{A}$ which satisfies the following properties. This creates a scalar addition analogue to scalar multiplication.
	\begin{enumerate}
		\item The action is
		\begin{enumerate}
			\item Free
			\begin{equation}
				\forall \vb{v}\in V: g \oplus \vb{v} = \vb{v} \implies g = 0
			\end{equation}
			\item Transitive
			\begin{equation}
				\forall \vb{u}, \vb{v} \in V: \exists g \in G: g \oplus \vb{u} = \vb{v}
			\end{equation}
		\end{enumerate}
	\end{enumerate}
\end{definition}

The definition above also implies the following properties

\begin{flalign}
	&& \forall g \in G, g \oplus \vb{0} &= \vb{0} & \text{Right identity. Proof \ref{proof:aff_ri}} \label{equation:aff_ri} \\
	&& \forall g \in G,  \vb{u}, \vb{v} \in V: a \oplus (\vb{u} + \vb{v}) &= (a \oplus \vb{u}) + \vb{v} & \text{Right compatibility. Proof \ref{proof:aff_ass}} \label{equation:aff_ass} \\
	&& a &= b & \text{Bijection. Proof \ref{proof:aff_bij}} \label{equation:aff_bij}
\end{flalign}

\begin{definition}[Euclidean Point Space]
	The Euclidean point space
\end{definition}

\begin{definition}[Euclidean Vector Space]
	content...
\end{definition}

\section{Vector Spaces in Continuum Mechanics}

do a proper construction here

construction of euclidean geometry but don't get to lost

Rn is a Euclidean geometry, also cartesian coordinates

note on how velocity

also fields which are functions, algebra of fields is pointwise based, calculus is trickier

coordinate systems: cartesian, polar, spherical, navd88, wgs84

















\section{Getting Started}

For all intents and purposes of continuum mechanics, it is sufficient to start with a Euclidean space, usually two- or three-dimensional, described by a Cartesian coordinate system, because this can be used to describe the physical world at our scale quite well.

\subsection{Notation}

Scalars are simply written as a letter, usually lower-case ($a$). Vectors are bolded ($\vb{a}$). When hand-written, they may have an arrow on top ($\vec{a}$) or a single underline ($\underline{a}$). Unit vectors, with a magnitude 1, may be marked with a circumflex ($\vu{a}$). Higher-order tensors are bolded and usually upper-case ($\vb{A}$). When written, they may have a number of underlines equal to their order.

\section{Bases and Components}

We start with some right-handed orthonormal basis $\{\vu{e}_{i}\}_{i = 1}^{n}$ for an $n$-dimensional Euclidean vector space $\mathbb{R}^{n}$. If these align with a Cartesian coordinate frame, or when a Cartesian coordinate frame is defined to line up with them, they're called the canonical basis, i.e. the standard one. Using the geometric definitions for the dot and cross product, the following relations exist between the basis vectors, represented in shorthand by the Kronecker delta $\delta$ and Levi-Civita symbol $\epsilon$.

These products are both bilinear. The dot product commutes, and the cross product anti-commutes.

\begin{flalign}
	&& \vu{e}_{i} \vdot \vu{e}_{j} = \delta_{ij} &:= \begin{cases}
		1, & i = j, \\
		0, & i \neq j.
	\end{cases} & \label{equation:uvec_dot_uvec} \\
	&& \left( \vu{e}_{i} \cross \vu{e}_{j} \right) \vdot \vb{\hat{e}}_k = \epsilon_{ijk} &:= \begin{cases}
		1, & (i, j, k) = (1, 2, 3), (2, 3, 1), \mathrm{or} \,(3, 1, 2), \\
		-1, & (i, j, k) = (3, 2, 1), (2, 1, 3), \mathrm{or} \,(1, 3, 2), \\
		0, & \mathrm{otherwise.}
	\end{cases} & \label{equation:uvec_cross_uvec_dot_uvec}
\end{flalign}

Since any basis set is linearly independent and spans the vector space, any vector in the space may be represented in terms of such a basis. The coefficients of the linear combination are called components. It is important to note that these are not the same as coordinates, especially for non-Cartesian coordinate systems.

\begin{flalign}
	&& \vb{a} &= \sum_{i = 1}^{n} a_{i} \vu{e}_{i} & \label{equation:vec}
\end{flalign}

\subsection{Cobasis}

If the chosen basis is not orthonormal with respect to some inner product and corresponding induced norm, then there is some more nuance. Sidestepping the excess mental infrastructure of the dual space and Riesz representation, the cobasis of some basis $\{\vb{f}_{i}\}_{i = 1}^{n}$, marked with a superscript as $\{\vb{f}^{i}\}_{i = 1}^{n}$, is another basis for the same space, which is bi-orthonormal to the basis.

\begin{flalign}
	&& \vb{f}^{i} \vdot \vb{f}_{j} = \vb{f}_{j} \vdot \vb{f}^{i} &= \delta_{ij} &
\end{flalign}

Notation for components in a non-orthonormal basis is more complicated. The contravariant components of the basis use superscripts, and the covariant components of the cobasis use subscripts. This may seem backwards, but this is the convention that was developed.

\begin{flalign}
	&& \vb{a} &= \sum_{i = 1}^{n} \tilde{a}^{i} \vb{f}_{i} & \label{equation:vec_contra} \\
	&& \vb{a} &= \sum_{i = 1}^{n} \tilde{a}_{i} \vb{f}^{i} & \label{equation:vec_co}
\end{flalign}

Contra in contravariant refers to how the components scale inversely with the basis vectors. For example, if a basis is changed from inches to feet, the components used to represent a vector will shrink to represent the same length. On the other hand, the covariant components would increase.

If one goes further into the weeds of tensor-world, there is a whole shorthand system of upstairs and downstairs indices used for tensors defined by a certain combination of bases and cobases. However, in continuum-world, one can fudge over all of these nuances by choosing to use an orthonormal basis, which is conveniently its own cobasis.

\subsection{Independence of Basis}

That freedom of choice is an important idea behind this tensor system. Up until this point, you were probably used to seeing vectors and matrices as lists and tables of numbers. Start with a unit vector $(1, 0)$. Rotate the coordinate frame 45 degrees clockwise and you get $(\sqrt{2} / 2, \sqrt{2} / 2)$. This would seem like a new and unequal vector, since $(1, 0) \neq (\sqrt{2} / 2, \sqrt{2} / 2)$.

Writing them in terms of components of particular bases allows for more abstraction and flexibility. The vector is no longer the tuple per se, but is represented by a tuple in the context of a particular reference frame. The vector and the physical quantity it represents stay the same conceptually, regardless of the frame of reference: $\vb{a} = \sum_{i = 1}^{n} a_{i} \vu{e}_{i} = \sum_{i = 1}^{n} \tilde{a}^{i} \vb{f}_{i} = \sum_{i = 1}^{n} \tilde{a}_{i} \vb{f}^{i}$. Only components vary. So we can say for the prior example, labeling the coordinate frames as A and B, $(1, 0)_{A} = (\sqrt{2} / 2, \sqrt{2} / 2)_{B}$. Operations and properties like an inner product, cross product, norm should stay the same, too. However, it may not necessarily be that $\vb{a} \vdot \vb{b} = \sum_{i = 1}^{n} \tilde{a}^{i} \tilde{b}^{i}$, because the dot product is defined geometrically, not algebraically.

\subsection{Computing Components}

differentiate coordinates and components

\subsection{Curvilinear Coordinates}

same vector using different coordinates

\section{Kronecker Delta and Levi-Civita Symbol Identities}

The Kronecker Delta and Levi-Civita Symbol have the following identities which are useful in proof.

\begin{flalign}
	&& a &= b & \text{Proof \ref{proof:aaa}} \label{equation:aaa} \\
	&& a &= b & \text{Proof \ref{proof:bbb}} \label{equation:bbb} \\
	&& \epsilon_{pqs} \epsilon_{nrs} &= \delta_{pn} \delta_{qr} - \delta_{pr} \delta_{qn} & \text{Proof \ref{proof:eps_pqs_eps_nrs}} \label{equation:eps_pqs_eps_nrs} \\
	&& \epsilon_{pqs} \epsilon_{rqs} &= 2 \delta_{pr} & \text{Proof \ref{proof:eps_pqs_eps_rqs}} \label{equation:eps_pqs_eps_rqs}
\end{flalign}

\section{Einstein Notation}