\chapter{Vectors}
\label{chapter:Vectors}

%&& a &= b & \text{Proof \ref{proof:}} \label{equation:}

Ironically enough, this was the hardest section to write. It is easy to get lost in abstraction and pedantics.

I think that the best way to start is with vectors. Tesors are always defined relative to some kind of vector space. Scalars are so-called zeroth-order tensors, vectors are so-called first-order tensors, and any other kind of tensor can be constructed from these.

\section{Vector Space Analysis}

\subsection{Vector Space}

scalar, tuples, functions

\begin{definition}[Abelian Group]
	An Abelian group $\{A, \cdot\}$ is a set $A$ and a binary operation $\cdot: A \times A \rightarrow A$ which satisfy the following four properties. Essentially, it generalizes the properties of addition and multiplication.
	\begin{enumerate}
		\item Commutativity: reversing the arguments does not change the result.
		\begin{equation}
			\forall a, b \in A: a \cdot b = b \cdot a
		\end{equation}
		\item Associativity: the order in which the operation is applied does not change the result.
		\begin{equation}
			\forall a, b, c \in A: a \cdot (b \cdot c) = (a \cdot b) \cdot c
		\end{equation}
		\item Identity element: any element operated on with the identity results in itself.
		\begin{equation}
			\exists i \in A: \forall a \in A: i \cdot a = a
		\end{equation}
		\item Inverse element: every element has a corresponding inverse which results in the identity.
		\begin{equation}
			\forall a \in A: \exists b \in A: a \cdot b = i
		\end{equation}
	\end{enumerate}
\end{definition}

\begin{definition}[Field]
	A field $\{ \mathbb{F}, +, \cdot \}$ is a set $\mathbb{F}$ with an addition-like operation $+: A \times A: A$ and a multiplication-like operation $+: A \times A: A$ which satisfy the following properties.
	\begin{enumerate}
		\item $\{ \mathbb{F}, + \}$ form an Abelian group, with an identity $0$ and additive inverse marked as $-a$.
		\item $\{ \mathbb{F} \setminus 0, \cdot \}$ form an Abelian group, with an identity $1$ and multiplicative inverse marked as $a^{-1}$.
		\item Distributivity of multiplication
		\begin{equation}
			\forall a, b, c \in A: a \cdot (b + c) = a \cdot b + a \cdot c
		\end{equation}
	\end{enumerate}
\end{definition}

For a field, the subtraction and division operations can also be defined.

\begin{align}
	a - b &:= a + (-b) \\
	a / b &:= a \cdot b^{-1}
\end{align}

\begin{definition}[Vector Space]
	A vector space $X$ over a field $\mathbb{F}$ is a system of sets and operations $\{X, \mathbb{F}, \boldsymbol{+}, +, \cdot, *\}$ which satisfy the following axioms.
	\begin{enumerate}
		\item $\{X, \boldsymbol{+}\}$ is an Abelian group with identity $\vb{0}$.
		\item $\{\mathbb{F}, +, \cdot\}$ is a field.
		\item Scalar multiplication $* : \mathbb{F} \times X \rightarrow X$ is a binary operation between the field and vector space which produces a vector, with the following properties.
		\begin{enumerate}
			\item Distributivity over vector addition
			\begin{equation}
				\forall \alpha \in \mathbb{F}, x, y \in X: \alpha * (\vb{x} \boldsymbol{+} \vb{y}) = \alpha * \vb{x} \boldsymbol{+} \alpha * \vb{y}
			\end{equation}
			\item Distributivity over field addition into vector addition
			\begin{equation}
				\forall \alpha, \beta \in \mathbb{F}, \vb{x} \in A: (\alpha + \beta) * \vb{x} = \alpha * \vb{x} \boldsymbol{+} \beta * \vb{x}
			\end{equation}
			\item Associativity with field multiplication into scalar multiplication
			\begin{equation}
				\forall \alpha, \beta \in \mathbb{F}, \vb{x} \in X: (\alpha \cdot \beta) * \vb{x} = \alpha * (\beta * \vb{x})
			\end{equation}
			\item Field identity is scalar multiplication identity
			\begin{equation}
				\forall \vb{x} \in X: 1 * \vb{x} = \vb{x}
			\end{equation}
		\end{enumerate}
	\end{enumerate}
\end{definition}

can become looser with notation??

additional properties to get identity and inverse vectors

\begin{itemize}
	\item $\forall \vb{x} \in X: 0 * \vb{x} = \vb{0}$
	\item $\forall \vb{x} \in X: (-1) * \vb{x} = -\vb{x}$
\end{itemize}

\subsection{Basis}

different kinds of bases, linear independence

\subsection{Metric}

\subsection{Norm}

metric norm back and forth

\subsection{Inner Product}

norm inner product back and forth

\subsection{Dual Space}

\subsection{Riesz Representation}

Riesz representation and dealing with complex vector spaces

\section{Inner Product and Norm}

norm is also a metric

\section{Vector Spaces in Continuum Mechanics}

do a proper construction here

construction of euclidean geometry but don't get to lost

Rn is a Euclidean geometry, also cartesian coordinates

note on how velocity

also fields which are functions

















\section{Getting Started}

For all intents and purposes of continuum mechanics, it is sufficient to start with a Euclidean space, usually two- or three-dimensional, described by a Cartesian coordinate system, because this can be used to describe the physical world at our scale quite well.

\subsection{Notation}

Scalars are simply written as a letter, usually lower-case ($a$). Vectors are bolded ($\vb{a}$). When hand-written, they may have an arrow on top ($\vec{a}$) or a single underline ($\underline{a}$). Unit vectors, with a magnitude 1, may be marked with a circumflex ($\vu{a}$). Higher-order tensors are bolded and usually upper-case ($\vb{A}$). When written, they may have a number of underlines equal to their order.

\section{Bases and Components}

We start with some right-handed orthonormal basis $\{\vu{e}_{i}\}_{i = 1}^{n}$ for an $n$-dimensional Euclidean vector space $\mathbb{R}^{n}$. If these align with a Cartesian coordinate frame, or when a Cartesian coordinate frame is defined to line up with them, they're called the canonical basis, i.e. the standard one. Using the geometric definitions for the dot and cross product, the following relations exist between the basis vectors, represented in shorthand by the Kronecker delta $\delta$ and Levi-Civita symbol $\epsilon$.

These products are both bilinear. The dot product commutes, and the cross product anti-commutes.

\begin{flalign}
	&& \vu{e}_{i} \vdot \vu{e}_{j} = \delta_{ij} &:= \begin{cases}
		1, & i = j, \\
		0, & i \neq j.
	\end{cases} & \label{equation:uvec_dot_uvec} \\
	&& \left( \vu{e}_{i} \cross \vu{e}_{j} \right) \vdot \vb{\hat{e}}_k = \epsilon_{ijk} &:= \begin{cases}
		1, & (i, j, k) = (1, 2, 3), (2, 3, 1), \mathrm{or} \,(3, 1, 2), \\
		-1, & (i, j, k) = (3, 2, 1), (2, 1, 3), \mathrm{or} \,(1, 3, 2), \\
		0, & \mathrm{otherwise.}
	\end{cases} & \label{equation:uvec_cross_uvec_dot_uvec}
\end{flalign}

Since any basis set is linearly independent and spans the vector space, any vector in the space may be represented in terms of such a basis. The coefficients of the linear combination are called components. It is important to note that these are not the same as coordinates, especially for non-Cartesian coordinate systems.

\begin{flalign}
	&& \vb{a} &= \sum_{i = 1}^{n} a_{i} \vu{e}_{i} & \label{equation:vec}
\end{flalign}

\subsection{Cobasis}

If the chosen basis is not orthonormal with respect to some inner product and corresponding induced norm, then there is some more nuance. Sidestepping the excess mental infrastructure of the dual space and Riesz representation, the cobasis of some basis $\{\vb{f}_{i}\}_{i = 1}^{n}$, marked with a superscript as $\{\vb{f}^{i}\}_{i = 1}^{n}$, is another basis for the same space, which is bi-orthonormal to the basis.

\begin{flalign}
	&& \vb{f}^{i} \vdot \vb{f}_{j} = \vb{f}_{j} \vdot \vb{f}^{i} &= \delta_{ij} &
\end{flalign}

Notation for components in a non-orthonormal basis is more complicated. The contravariant components of the basis use superscripts, and the covariant components of the cobasis use subscripts. This may seem backwards, but this is the convention that was developed.

\begin{flalign}
	&& \vb{a} &= \sum_{i = 1}^{n} \tilde{a}^{i} \vb{f}_{i} & \label{equation:vec_contra} \\
	&& \vb{a} &= \sum_{i = 1}^{n} \tilde{a}_{i} \vb{f}^{i} & \label{equation:vec_co}
\end{flalign}

Contra in contravariant refers to how the components scale inversely with the basis vectors. For example, if a basis is changed from inches to feet, the components used to represent a vector will shrink to represent the same length. On the other hand, the covariant components would increase.

If one goes further into the weeds of tensor-world, there is a whole shorthand system of upstairs and downstairs indices used for tensors defined by a certain combination of bases and cobases. However, in continuum-world, one can fudge over all of these nuances by choosing to use an orthonormal basis, which is conveniently its own cobasis.

\subsection{Independence of Basis}

That freedom of choice is an important idea behind this tensor system. Up until this point, you were probably used to seeing vectors and matrices as lists and tables of numbers. Start with a unit vector $(1, 0)$. Rotate the coordinate frame 45 degrees clockwise and you get $(\sqrt{2} / 2, \sqrt{2} / 2)$. This would seem like a new and unequal vector, since $(1, 0) \neq (\sqrt{2} / 2, \sqrt{2} / 2)$.

Writing them in terms of components of particular bases allows for more abstraction and flexibility. The vector is no longer the tuple per se, but is represented by a tuple in the context of a particular reference frame. The vector and the physical quantity it represents stay the same conceptually, regardless of the frame of reference: $\vb{a} = \sum_{i = 1}^{n} a_{i} \vu{e}_{i} = \sum_{i = 1}^{n} \tilde{a}^{i} \vb{f}_{i} = \sum_{i = 1}^{n} \tilde{a}_{i} \vb{f}^{i}$. Only components vary. So we can say for the prior example, labeling the coordinate frames as A and B, $(1, 0)_{A} = (\sqrt{2} / 2, \sqrt{2} / 2)_{B}$. Operations and properties like an inner product, cross product, norm should stay the same, too. However, it may not necessarily be that $\vb{a} \vdot \vb{b} = \sum_{i = 1}^{n} \tilde{a}^{i} \tilde{b}^{i}$, because the dot product is defined geometrically, not algebraically.

\subsection{Computing Components}

differentiate coordinates and components

\subsection{Curvilinear Coordinates}

same vector using different coordinates

\section{Kronecker Delta and Levi-Civita Symbol Identities}

The Kronecker Delta and Levi-Civita Symbol have the following identities which are useful in proof.

\begin{flalign}
	&& a &= b & \text{Proof \ref{proof:aaa}} \label{equation:aaa} \\
	&& a &= b & \text{Proof \ref{proof:bbb}} \label{equation:bbb} \\
	&& \epsilon_{pqs} \epsilon_{nrs} &= \delta_{pn} \delta_{qr} - \delta_{pr} \delta_{qn} & \text{Proof \ref{proof:eps_pqs_eps_nrs}} \label{equation:eps_pqs_eps_nrs} \\
	&& \epsilon_{pqs} \epsilon_{rqs} &= 2 \delta_{pr} & \text{Proof \ref{proof:eps_pqs_eps_rqs}} \label{equation:eps_pqs_eps_rqs}
\end{flalign}

\section{Einstein Notation}